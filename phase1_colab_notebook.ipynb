{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0c8d2c",
   "metadata": {},
   "source": [
    "# E-Commerce Review Analyzer - Phase 1: Core AI Logic\n",
    "\n",
    "**Project Goal:** Develop an AI system for automated e-commerce review analysis\n",
    "\n",
    "**Core AI Functions:**\n",
    "- Sentiment Analysis (Classification using DistilBERT)\n",
    "- Abstractive Summarization (Generative AI using T5)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db44b446",
   "metadata": {},
   "source": [
    "## Step 1.1: Environment Setup\n",
    "Install required libraries for transformers, torch, and data handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cc5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch pandas datasets matplotlib seaborn scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcadf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ“ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d9dd30",
   "metadata": {},
   "source": [
    "## Step 1.2: Data Ingestion\n",
    "Load open-source Amazon Reviews dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191266cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon Reviews dataset (using a subset for efficiency)\n",
    "# Using the Amazon US Reviews dataset from Hugging Face\n",
    "print(\"Loading dataset... This may take a moment.\")\n",
    "\n",
    "# Load a smaller subset: Amazon Polarity dataset (positive/negative reviews)\n",
    "dataset = load_dataset(\"amazon_polarity\", split=\"test[:5000]\")  # Load 5000 reviews\n",
    "\n",
    "# Convert to pandas DataFrame\n",
    "df = pd.DataFrame(dataset)\n",
    "\n",
    "# Rename columns for clarity\n",
    "df.columns = ['label', 'title', 'content']\n",
    "\n",
    "# Combine title and content for full review text\n",
    "df['review_text'] = df['title'] + \" \" + df['content']\n",
    "\n",
    "# Map labels: 0=Negative, 1=Positive (original dataset format)\n",
    "df['original_sentiment'] = df['label'].map({0: 'negative', 1: 'positive'})\n",
    "\n",
    "print(f\"âœ“ Loaded {len(df)} reviews successfully!\")\n",
    "print(f\"\\nDataset shape: {df.shape}\")\n",
    "print(f\"\\nColumn names: {df.columns.tolist()}\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16190241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic statistics\n",
    "print(\"Dataset Overview:\")\n",
    "print(f\"Total Reviews: {len(df)}\")\n",
    "print(f\"\\nSentiment Distribution:\")\n",
    "print(df['original_sentiment'].value_counts())\n",
    "print(f\"\\nAverage review length: {df['review_text'].str.len().mean():.0f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995abd26",
   "metadata": {},
   "source": [
    "## Step 1.3: Sentiment Analysis LLM (Classification)\n",
    "Implement sentiment classification using pre-trained DistilBERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e6505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained sentiment analysis model\n",
    "print(\"Loading sentiment analysis model (DistilBERT)...\")\n",
    "\n",
    "# Using distilbert-base-uncased-finetuned-sst-2-english for sentiment analysis\n",
    "sentiment_analyzer = pipeline(\n",
    "    \"sentiment-analysis\",\n",
    "    model=\"distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "print(\"âœ“ Sentiment model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2da7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentiment(text):\n",
    "    \"\"\"\n",
    "    Classify sentiment of a given text using DistilBERT.\n",
    "    \n",
    "    Args:\n",
    "        text (str): Review text to analyze\n",
    "    \n",
    "    Returns:\n",
    "        dict: Contains 'label' (POSITIVE/NEGATIVE) and 'score' (confidence)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Truncate text if too long\n",
    "        text = text[:512] if len(text) > 512 else text\n",
    "        \n",
    "        # Get prediction\n",
    "        result = sentiment_analyzer(text)[0]\n",
    "        \n",
    "        return {\n",
    "            'label': result['label'],\n",
    "            'score': round(result['score'], 4)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'label': 'NEUTRAL',\n",
    "            'score': 0.0,\n",
    "            'error': str(e)\n",
    "        }\n",
    "\n",
    "# Test the function\n",
    "test_text = \"This product is amazing! I love it so much.\"\n",
    "print(\"Test sentiment classification:\")\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Result: {classify_sentiment(test_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4988b83c",
   "metadata": {},
   "source": [
    "## Step 1.4: Summarization LLM (Generative AI)\n",
    "Implement abstractive summarization using pre-trained T5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9168c262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained summarization model\n",
    "print(\"Loading summarization model (T5)...\")\n",
    "\n",
    "# Using T5-small for efficient summarization\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    model=\"t5-small\",\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "print(\"âœ“ Summarization model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafdfca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(texts, max_length=150, min_length=40):\n",
    "    \"\"\"\n",
    "    Generate abstractive summary from a list of texts using T5.\n",
    "    \n",
    "    Args:\n",
    "        texts (list or str): Single text or list of texts to summarize\n",
    "        max_length (int): Maximum length of summary\n",
    "        min_length (int): Minimum length of summary\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated summary\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Handle single text or list\n",
    "        if isinstance(texts, str):\n",
    "            combined_text = texts\n",
    "        else:\n",
    "            combined_text = \" \".join(texts)\n",
    "        \n",
    "        # Truncate if too long (T5 has token limits)\n",
    "        if len(combined_text) > 1000:\n",
    "            combined_text = combined_text[:1000]\n",
    "        \n",
    "        # Generate summary\n",
    "        summary = summarizer(\n",
    "            combined_text,\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            do_sample=False\n",
    "        )\n",
    "        \n",
    "        return summary[0]['summary_text']\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "# Test the function\n",
    "test_reviews = [\n",
    "    \"This product exceeded my expectations. The quality is outstanding and shipping was fast.\",\n",
    "    \"Great value for money. Would definitely recommend to friends and family.\",\n",
    "    \"The customer service was excellent. They helped resolve my issue quickly.\"\n",
    "]\n",
    "print(\"Test summarization:\")\n",
    "print(f\"Number of reviews: {len(test_reviews)}\")\n",
    "print(f\"\\nGenerated Summary:\\n{generate_summary(test_reviews)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fea7532",
   "metadata": {},
   "source": [
    "## Step 1.5: Functional Proof - Test on 10 Random Reviews\n",
    "Validate both LLM functions on sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2551ad08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 10 random reviews for testing\n",
    "test_sample = df.sample(n=10, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FUNCTIONAL PROOF: Testing Both LLM Functions on 10 Random Reviews\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Test sentiment classification on each review\n",
    "print(\"\\n1. SENTIMENT ANALYSIS TEST:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "sentiment_results = []\n",
    "for idx, row in test_sample.iterrows():\n",
    "    review_text = row['review_text'][:200]  # Show first 200 chars\n",
    "    sentiment = classify_sentiment(row['review_text'])\n",
    "    sentiment_results.append(sentiment)\n",
    "    \n",
    "    print(f\"\\nReview {idx + 1}:\")\n",
    "    print(f\"Text: {review_text}...\")\n",
    "    print(f\"Original Label: {row['original_sentiment']}\")\n",
    "    print(f\"Predicted Sentiment: {sentiment['label']} (Confidence: {sentiment['score']})\")\n",
    "\n",
    "# Add results to dataframe\n",
    "test_sample['predicted_sentiment'] = [r['label'] for r in sentiment_results]\n",
    "test_sample['confidence_score'] = [r['score'] for r in sentiment_results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c095633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test summarization on positive and negative reviews separately\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"2. SUMMARIZATION TEST:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Get positive reviews from test sample\n",
    "positive_reviews = test_sample[test_sample['original_sentiment'] == 'positive']['review_text'].tolist()\n",
    "negative_reviews = test_sample[test_sample['original_sentiment'] == 'negative']['review_text'].tolist()\n",
    "\n",
    "print(f\"\\nðŸ“Š Positive Reviews ({len(positive_reviews)} reviews):\")\n",
    "if positive_reviews:\n",
    "    positive_summary = generate_summary(positive_reviews[:5])  # Summarize up to 5 reviews\n",
    "    print(f\"Summary: {positive_summary}\")\n",
    "else:\n",
    "    print(\"No positive reviews in sample.\")\n",
    "\n",
    "print(f\"\\nðŸ“Š Negative Reviews ({len(negative_reviews)} reviews):\")\n",
    "if negative_reviews:\n",
    "    negative_summary = generate_summary(negative_reviews[:5])  # Summarize up to 5 reviews\n",
    "    print(f\"Summary: {negative_summary}\")\n",
    "else:\n",
    "    print(\"No negative reviews in sample.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469fe54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display validation summary\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"VALIDATION SUMMARY:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nâœ“ Sentiment Classification Function: WORKING\")\n",
    "print(f\"  - Successfully processed {len(test_sample)} reviews\")\n",
    "print(f\"  - Output format: dict with 'label' and 'score' keys\")\n",
    "print(f\"  - Average confidence: {test_sample['confidence_score'].mean():.2%}\")\n",
    "\n",
    "print(\"\\nâœ“ Summarization Function: WORKING\")\n",
    "print(f\"  - Successfully generated summaries for positive and negative reviews\")\n",
    "print(f\"  - Output format: string (abstractive summary)\")\n",
    "\n",
    "print(\"\\nðŸŽ‰ PHASE 1 COMPLETE: Core AI Logic Validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8035a17b",
   "metadata": {},
   "source": [
    "## Bonus: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254a73c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sentiment distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original sentiment distribution\n",
    "test_sample['original_sentiment'].value_counts().plot(\n",
    "    kind='bar', \n",
    "    ax=axes[0], \n",
    "    color=['#ff6b6b', '#51cf66'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[0].set_title('Original Sentiment Labels', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Sentiment')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Predicted sentiment distribution\n",
    "test_sample['predicted_sentiment'].value_counts().plot(\n",
    "    kind='bar', \n",
    "    ax=axes[1], \n",
    "    color=['#ff6b6b', '#51cf66'],\n",
    "    alpha=0.7\n",
    ")\n",
    "axes[1].set_title('Predicted Sentiment (DistilBERT)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Sentiment')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display confidence score distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(test_sample['confidence_score'], bins=10, color='#4c6ef5', alpha=0.7, edgecolor='black')\n",
    "plt.title('Model Confidence Score Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Confidence Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(test_sample['confidence_score'].mean(), color='red', linestyle='--', label=f'Mean: {test_sample[\"confidence_score\"].mean():.3f}')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e406c3fb",
   "metadata": {},
   "source": [
    "## Save Processed Data for Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3437795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a subset of processed data for use in Streamlit dashboard\n",
    "# Process sentiment for 1000 reviews (more efficient for dashboard)\n",
    "sample_for_dashboard = df.sample(n=1000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "print(\"Processing sentiment for 1000 reviews... This may take a few minutes.\")\n",
    "\n",
    "# Process in batches\n",
    "sentiments = []\n",
    "for idx, text in enumerate(sample_for_dashboard['review_text']):\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx}/{len(sample_for_dashboard)} reviews...\")\n",
    "    sentiments.append(classify_sentiment(text))\n",
    "\n",
    "sample_for_dashboard['sentiment'] = [s['label'] for s in sentiments]\n",
    "sample_for_dashboard['confidence'] = [s['score'] for s in sentiments]\n",
    "\n",
    "# Save to CSV\n",
    "sample_for_dashboard.to_csv('processed_reviews.csv', index=False)\n",
    "print(\"\\nâœ“ Saved processed_reviews.csv for Phase 2 dashboard!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
